This repository is a suite of command-line utilities for extracting certain
information from Wikipedia. The following tools are included:

- `get_sents_refs.py`: Extract sentences and any URLs referenced in a citation for
  each sentence

# The `get_sents_refs.py` script

Extract sentences and any URLs referenced in a citation for each sentence in a
Wikipedia page.

This script is called from the command line with a Wikipedia page title passed
as an argument. Note that the `-l` option allows the user to specify a foreign
language version of the page, but the English title must still be specified.

The wikitext returned by the Wikipedia API is then parsed and mined for
sentences. The result is printed out to `stdout` in plain text, with one
sentence per line, and any found reference urls for that line appended and
separated by tab characters.

Usage:

```bash
usage: get_sents_refs.py [-h] [-l LANGUAGE] [--quoted] english_title

positional arguments:
  english_title         the title of the page on the English Wikipedia site to
                        be processed

optional arguments:
  -h, --help            show this help message and exit
  -l LANGUAGE, --language LANGUAGE
                        the language of the Wikipedia site. Choices include
                        en, es, etc.
  --quoted              the title of the Wikipedia page already has special
                        characters replaced with the %xx escape
```


## Requirements


### Python packages

To easily install all the required Python packages, (create a new `virtualenv`
and) run the command `pip install -r requirements.txt` from the root directory
of this repository.


### Other tools


#### Splitta

Install Splitta from TBD. If the `splitta` command line tool is not installed
such that it can be found in the system `PATH`, then export the environment
variable `SPLITTA_BIN` with its value the path to the `splitta` binary.


## Caveats

The data generated by the `get_sents_refs.py` script currently does not expand all
wikitext templates to the text that would be displayed when viewing the page in
a web browser. For example, a template like `{{convert|130|ft|m}}` would be
expanded to `130 feet (40&nbsp;m)`.


## TODO

- Convert html entities to unicode:
  [http://stackoverflow.com/questions/701704/convert-html-entities-to-unicode-and-vice-versa
  ](http://stackoverflow.com/questions/701704/convert-html-entities-to-unicode-and-vice-versa)
- Extract any URLs found between `<ref></ref>` tags (John Gruber provides a
  regexp), not just those found as the value of a `| url = http://...` attribute
  of a `{{cite ... }}` or `{{Citation ... }}` template.
- Fork the GitHub string_scanner repository. Then make it an installable
  package so that it can be installed with pip.


## Notes

- The `strip_code` method in the `mwparserfromhell` package is not implemented.
  More info: [https://github.com/earwig/mwparserfromhell/issues/9
  ](https://github.com/earwig/mwparserfromhell/issues/9)
- To expand templates, add the `&rvexpandtemplates=` parameter to the API call.
- On citations:
  - [Mediawiki documentation](https://www.mediawiki.org/wiki/Extension:Cite/Cite.php)
    - Text between `<ref></ref>` tags doesn't get rendered in line (or anywhere)
  - https://en.wikipedia.org/wiki/Wikipedia:Inline_citation
  - https://en.wikipedia.org/wiki/Wikipedia:Citation_templates
  - https://en.wikipedia.org/wiki/Help:Footnotes
- Python String Scanner [source](https://github.com/markwatkinson/python-string-scanner)
  and [documentation](http://asgaard.co.uk/p/Python-StringScanner)
- [MediaWiki parsers](https://www.mediawiki.org/wiki/Alternative_parsers)
